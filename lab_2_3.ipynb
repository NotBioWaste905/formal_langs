{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920a129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3f057",
   "metadata": {},
   "source": [
    "# Контекстно-свободная грамматика для toki pona\n",
    "\n",
    "Описывать будем toki pona, в частности именные группы (с модификаторами) и простые предложения (с прямым объектом). Также я сделаю одно важно допущение: в toki pona всё достаточно сложно с частями речи, т.к. языковые единицы не могут изменяться одна и та же единица может принимать значения разных частей речи в зависимости от своей позиции. Например:\n",
    "\n",
    "| word   | meaning                                  |\n",
    "|--------|------------------------------------------|\n",
    "| `telo` | вода, жидкость, мокрое, пить, мыть, течь |\n",
    "| `moku` | еда, есть                                |\n",
    "| `ona`  | местоимение третьего лица                |\n",
    "| `e`    | показатель прямого объекта               |\n",
    "| `li`   | показатель предиката (глагола)           |\n",
    "\n",
    "ona li telo - они пьют\n",
    "\n",
    "ona li moku - они едят\n",
    "\n",
    "ona li moku e moku telo - они едят мокрую еду (суп, например)\n",
    "\n",
    "ona li telo e telo - они пьют воду\n",
    "\n",
    "Для простоты (хотя интересно об этом потом подумать) мы возьмем ограниченный корпус toki pona \"со снятой омонимией\", где будем использовать каждую единицу в качестве только одной части речи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156a705",
   "metadata": {},
   "source": [
    "```bnf\n",
    "start: sentence\n",
    "\n",
    "sentence: context_phrase? subject_part predicate\n",
    "context_phrase: np \"la\"\n",
    "subject_part: subject \"li\"?\n",
    "subject: np (\"en\" np)*\n",
    "predicate: verb_phrase (object | prep_phrase)*\n",
    "verb_phrase: PREVERB? VERB modifier_phrase?\n",
    "object: \"e\" np\n",
    "prep_phrase: PREPOSITION_MOD np\n",
    "\n",
    "np: (NOUN | PRONOUN) modifier_phrase?\n",
    "modifier_phrase: (modifier | pi_phrase)+\n",
    "pi_phrase: \"pi\" np\n",
    "modifier: NOUN\n",
    "\n",
    "PREPOSITION_MOD: \"kepeken\" | \"lon\" | \"tawa\" | \"tan\" | \"sama\"\n",
    "PREVERB: \"awen\" | \"kama\" | \"ken\" | \"lukin\" | \"sona\" | \"wile\" | \"alasa\"\n",
    "PRONOUN: \"mi\" | \"sina\" | \"ona\"\n",
    "NOUN: \"telo\" | \"ilo\" | \"jan\" | \"kala\" | \"kasi\" | \"tomo\" | \"moku\" | \"soweli\" | \"esun\" | \"jo\" | \"kalama\" | \"toki\" | \"kili\" | \"suli\" | \"lili\" | \"pona\"\n",
    "VERB: \"esun\" | \"jo\" | \"kalama\" | \"toki\"\n",
    "\n",
    "%import common.WS\n",
    "%ignore WS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44847459",
   "metadata": {},
   "source": [
    "Попробуем помучать LARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b8d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269a08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "  sentence\n",
      "    subject_part\n",
      "      subject\n",
      "        np\tmi\n",
      "    predicate\n",
      "      verb_phrase\ttoki\n",
      "      object\n",
      "        np\tona\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toki_pona_grammar = r\"\"\"\n",
    "start: sentence\n",
    "\n",
    "sentence: context_phrase? subject_part predicate\n",
    "context_phrase: np \"la\"\n",
    "subject_part: subject \"li\"?\n",
    "subject: np (\"en\" np)*\n",
    "predicate: verb_phrase (object | prep_phrase)*\n",
    "verb_phrase: PREVERB? VERB modifier_phrase?\n",
    "object: \"e\" np\n",
    "prep_phrase: PREPOSITION_MOD np\n",
    "\n",
    "np: (NOUN | PRONOUN) modifier_phrase?\n",
    "modifier_phrase: (modifier | pi_phrase)+\n",
    "pi_phrase: \"pi\" np\n",
    "modifier: NOUN\n",
    "\n",
    "PREPOSITION_MOD: \"kepeken\" | \"lon\" | \"tawa\" | \"tan\" | \"sama\"\n",
    "PREVERB: \"awen\" | \"kama\" | \"ken\" | \"lukin\" | \"sona\" | \"wile\" | \"alasa\"\n",
    "PRONOUN: \"mi\" | \"sina\" | \"ona\"\n",
    "NOUN: \"telo\" | \"ilo\" | \"jan\" | \"kala\" | \"kasi\" | \"tomo\" | \"moku\" | \"soweli\" | \"esun\" | \"jo\" | \"kalama\" | \"toki\" | \"kili\" | \"suli\" | \"lili\" | \"pona\"\n",
    "VERB: \"esun\" | \"jo\" | \"kalama\" | \"toki\" | \"moku\"\n",
    "\n",
    "%import common.WS\n",
    "%ignore WS\n",
    "\n",
    "\"\"\"\n",
    "parser = Lark(toki_pona_grammar, start=\"start\")\n",
    "tree = parser.parse(\"mi toki e ona\")  # I am talking to him/her/them\n",
    "print(tree.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db149ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "  sentence\n",
      "    subject_part\n",
      "      subject\n",
      "        np\tona\n",
      "    predicate\n",
      "      verb_phrase\tmoku\n",
      "      object\n",
      "        np\tmi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = parser.parse(\"ona li moku e mi\")  # it is eating me (so sad)\n",
    "print(tree.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25128875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "  sentence\n",
      "    subject_part\n",
      "      subject\n",
      "        np\tona\n",
      "    predicate\n",
      "      verb_phrase\tmoku\n",
      "      object\n",
      "        np\n",
      "          moku\n",
      "          modifier_phrase\n",
      "            modifier\ttelo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = parser.parse(\"ona li moku e moku telo\")  # they are eating a soup\n",
    "print(tree.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6ef222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I, me, we, us', 'speak, say, communicate', 'he, she, it, they, them']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lark import Visitor, Token\n",
    "\n",
    "\n",
    "class Translator(Visitor):\n",
    "    def __init__(self):\n",
    "        self.leaves = []\n",
    "        self.toki_pona_dict = {\n",
    "            \"PREPOSITION_MOD\": {\n",
    "                \"kepeken\": \"using, with\",\n",
    "                \"lon\": \"in, at, on\",\n",
    "                \"tawa\": \"to, for, toward\",\n",
    "                \"tan\": \"from, because of\",\n",
    "                \"sama\": \"like, same as\",\n",
    "            },\n",
    "            \"PREVERB\": {\n",
    "                \"awen\": \"keep, stay\",\n",
    "                \"kama\": \"become, come\",\n",
    "                \"ken\": \"can, may\",\n",
    "                \"lukin\": \"see, look (attempt)\",\n",
    "                \"sona\": \"know, know how to\",\n",
    "                \"wile\": \"want, need, must\",\n",
    "                \"alasa\": \"hunt, seek\",\n",
    "            },\n",
    "            \"PRONOUN\": {\n",
    "                \"mi\": \"I, me, we, us\",\n",
    "                \"sina\": \"you\",\n",
    "                \"ona\": \"he, she, it, they, them\",\n",
    "            },\n",
    "            \"NOUN\": {\n",
    "                \"telo\": \"water, liquid\",\n",
    "                \"ilo\": \"tool, machine\",\n",
    "                \"jan\": \"person, people\",\n",
    "                \"kala\": \"fish, sea creature\",\n",
    "                \"kasi\": \"plant, herb\",\n",
    "                \"tomo\": \"house, building, room\",\n",
    "                \"moku\": \"food, meal\",\n",
    "                \"soweli\": \"animal, land mammal\",\n",
    "                \"esun\": \"market, shop, trade\",\n",
    "                \"jo\": \"possession, having\",\n",
    "                \"kalama\": \"sound, noise\",\n",
    "                \"toki\": \"language, speech\",\n",
    "                \"kili\": \"fruit, vegetable\",\n",
    "                \"suli\": \"size, greatness\",\n",
    "                \"lili\": \"smallness, fewness\",\n",
    "                \"pona\": \"good, simplicity\",\n",
    "            },\n",
    "            \"VERB\": {\n",
    "                \"esun\": \"trade, buy, sell\",\n",
    "                \"jo\": \"have, possess\",\n",
    "                \"kalama\": \"make noise, play an instrument\",\n",
    "                \"toki\": \"speak, say, communicate\",\n",
    "                \"moku\": \"eat, drink, consume\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __default__(self, tree):\n",
    "        for child in tree.children:\n",
    "            if isinstance(child, Token):\n",
    "                self.leaves.append((child.type, child.value))\n",
    "\n",
    "    def translate(self, sentence, parser, merge=True):\n",
    "        tree = parser.parse(sentence)\n",
    "        self.visit_topdown(tree)\n",
    "        if merge:    \n",
    "            return \" \".join(\n",
    "                self.toki_pona_dict.get(pos, {}).get(word, word)\n",
    "                for pos, word in self.leaves\n",
    "            )\n",
    "        else:\n",
    "            return [self.toki_pona_dict.get(pos, {}).get(word, word) for pos, word in self.leaves]\n",
    "\n",
    "\n",
    "parser = Lark(toki_pona_grammar)\n",
    "\n",
    "# Visit the tree and collect tokens in order\n",
    "translator = Translator()\n",
    "\n",
    "translator.translate(\"mi toki e ona\", parser, merge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3047744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I, me, we, us', 'eat, drink, consume', 'water, liquid']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator()\n",
    "\n",
    "# Ошибка, нельзя ставить li после mi\n",
    "translator.translate(\"mi li moku telo\", parser, merge=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c36f4",
   "metadata": {},
   "source": [
    "Окей, он выдает нам какой-то набор слов, это уже неплохо. Интересно, сможем ли мы перевести эти \"лемматизированные\" предложения в нормальный английский язык? С учетом того как мал корпус токи поны, это может быть одним из вариантов перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b4b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bbafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith('N'):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, 'n'))\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, 'v'))\n",
    "        elif tag.startswith('J'):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, 'a'))\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, 'r'))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Load a dataset like WikiText or a subset of books\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a72c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:54<00:00, 1842.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dataset = dataset.shuffle(seed=42).select(range(100000))  # Use a smaller subset for testing\n",
    "\n",
    "training_pairs = []\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    if text.strip():  # Skip empty lines\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if 5 <= len(sentence.split()) <= 20:  # Filter by length\n",
    "                lemmatized = lemmatize_sentence(sentence)\n",
    "                # if lemmatized != sentence:  # Only include if actually changed\n",
    "                training_pairs.append({\"lemmatized\": lemmatized, \"original\": sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b73a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaae4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"inflect: \" + ex for ex in examples[\"lemmatized\"]]\n",
    "    targets = examples[\"original\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324c5a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80000/80000 [00:17<00:00, 4565.71 examples/s]\n",
      "Map: 100%|██████████| 22089/22089 [00:04<00:00, 4643.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(training_pairs[:80000])\n",
    "val_dataset = Dataset.from_list(training_pairs[80000:])\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b561bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da783943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   51/15000 00:19 < 1:37:41, 2.55 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/transformers/trainer.py:2207\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2205\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/transformers/trainer.py:2549\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2542\u001b[39m context = (\n\u001b[32m   2543\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2544\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2547\u001b[39m )\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2552\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2554\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2555\u001b[39m ):\n\u001b[32m   2556\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2557\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/transformers/trainer.py:3798\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3796\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2553\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Документы/HSE/formal_langs/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "formal-langs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
